#!/bin/bash
#
# 03_balanced_optimizer.sh - Balanced Atlas-Metric Optimization fÃ¼r Soccer Study
# =============================================================================
#
# Dieses Skript ist Schritt 03 und fÃ¼hrt den balancierten Optimierungsansatz aus:
# 1. Avoids FA dominance (extreme effect sizes)
# 2. Prioritizes moderate effect sizes (0.8-3.0 Cohen's d)
# 3. Favors COUNT/NCOUNT2 metrics for biological plausibility
# 4. Maintains scientific rigor for group comparisons
#
# Author: Generated for professional female soccer player connectivity study
# Date: 2025-08-08
#

set -e

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
OUTPUT_DIR="balanced_optimization_results"
TIMESTAMP=$(date "+%Y%m%d_%H%M%S")

echo "ğŸ¯ Balanced Atlas-Metric Optimization fÃ¼r Soccer Study (Schritt 03)"
echo "======================================================"
echo "Starting balanced optimization at $(date)"
echo ""

# Check prerequisites (no installations here; use 00_install.sh for setup)
echo "ğŸ“‹ Checking prerequisites..."

if [ ! -d "optimization_results" ]; then
	echo "âŒ Error: No optimization_results directory found."
	echo "   Bitte zuerst 02_compute_graph_metrics.sh ausfÃ¼hren."
	exit 1
fi

# Find the latest optimization results
LATEST_OPT_DIR=$(find optimization_results -name "optimization_*" -type d | sort | tail -1)
if [ -z "$LATEST_OPT_DIR" ]; then
	echo "âŒ Error: No optimization results found in optimization_results/"
	exit 1
fi


OPTIMIZED_METRICS_FILE="$LATEST_OPT_DIR/optimized_metrics.csv"
if [ ! -f "$OPTIMIZED_METRICS_FILE" ]; then
	echo "âŒ Error: optimized_metrics.csv nicht gefunden in $LATEST_OPT_DIR"
	exit 1
fi

# PrÃ¼fe, ob mindestens 4 verschiedene Subjekte vorhanden sind
SUBJECT_COUNT=$(awk -F',' 'NR>1{print $1}' "$OPTIMIZED_METRICS_FILE" | sort | uniq | wc -l | tr -d ' ')
if [ "$SUBJECT_COUNT" -lt 4 ]; then
	echo "âŒ Fehler: FÃ¼r eine sinnvolle statistische Analyse mÃ¼ssen mindestens 4 verschiedene Subjekte vorhanden sein."
	echo "   Aktuell gefunden: $SUBJECT_COUNT. Bitte bereiten Sie mehr Daten vor, bevor Sie fortfahren."
	exit 1
fi

echo "âœ… Gefundene Optimierungsergebnisse: $LATEST_OPT_DIR"
echo "âœ… Verwende Metrikdatei: $OPTIMIZED_METRICS_FILE"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Create the balanced optimization Python script inline
cat > "${OUTPUT_DIR}/balanced_optimizer_${TIMESTAMP}.py" << 'EOF'
#!/usr/bin/env python3
"""
Balanced Atlas-Metric Optimization (Generated by 03_balanced_optimizer.sh)
"""

import pandas as pd
import numpy as np
import sys
import json
from datetime import datetime
from pathlib import Path

def apply_balanced_scoring(df):
	"""Apply balanced scoring that penalizes FA dominance."""
    
	df_balanced = df.copy()
    
	def calculate_balanced_score(row):
		original_score = row['quality_score']
		metric = row['connectivity_metric']
        
		# FA penalty (extreme effect sizes)
		fa_penalty = 0.3 if metric == 'fa' else 1.0
        
		# Dominance penalty for very high scores
		if original_score >= 0.95:
			dominance_penalty = 0.4
		elif original_score >= 0.90:
			dominance_penalty = 0.6
		elif original_score >= 0.80:
			dominance_penalty = 0.8
		else:
			dominance_penalty = 1.0
        
		# Balance bonus for moderate metrics
		if metric in ['count', 'ncount2']:
			balance_bonus = 1.2
		elif metric == 'qa':
			balance_bonus = 1.1
		else:
			balance_bonus = 1.0
        
		balanced_score = original_score * fa_penalty * dominance_penalty * balance_bonus
		return min(balanced_score, 1.0)
    
	df_balanced['balanced_score'] = df_balanced.apply(calculate_balanced_score, axis=1)
    
	# Add rationale
	def get_rationale(row):
		metric = row['connectivity_metric']
		original = row['quality_score']
        
		reasons = []
		if metric == 'fa':
			reasons.append("FA penalty (extreme effects)")
		elif metric in ['count', 'ncount2']:
			reasons.append("COUNT/NCOUNT2 bonus (moderate effects)")
        
		if original >= 0.95:
			reasons.append("High dominance penalty")
		elif original >= 0.90:
			reasons.append("Moderate dominance penalty")
        
		return "; ".join(reasons) if reasons else "Optimal range maintained"
    
	df_balanced['scoring_rationale'] = df_balanced.apply(get_rationale, axis=1)
	return df_balanced

def main():
	if len(sys.argv) != 2:
		print("Usage: python script.py <optimized_metrics.csv>")
		sys.exit(1)
    
	input_file = sys.argv[1]
    
	# Load data
	print(f"ğŸ“Š Loading optimization results from: {input_file}")
	df = pd.read_csv(input_file)
	print(f"   Original combinations: {len(df)}")
    
	# Apply balanced scoring
	print("âš–ï¸  Applying balanced scoring...")
	df_balanced = apply_balanced_scoring(df)
    
	# Sort by balanced score
	df_sorted = df_balanced.sort_values('balanced_score', ascending=False)
    
	# Generate summary
	summary = {
		'total_combinations': len(df_balanced),
		'fa_combinations': len(df_balanced[df_balanced['connectivity_metric'] == 'fa']),
		'count_ncount2_combinations': len(df_balanced[df_balanced['connectivity_metric'].isin(['count', 'ncount2'])]),
		'mean_balanced_score': df_balanced['balanced_score'].mean(),
		'top_metric_distribution': df_sorted.head(20)['connectivity_metric'].value_counts().to_dict()
	}
    
	# Save results
	timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
	# Detailed CSV
	output_csv = f"balanced_optimization_results_{timestamp}.csv"
	df_balanced.to_csv(output_csv, index=False)
	print(f"âœ… Saved detailed results: {output_csv}")
    
	# Top recommendations JSON
	top_10 = []
	for i, (_, row) in enumerate(df_sorted.head(10).iterrows()):
		top_10.append({
			'rank': i + 1,
			'atlas': row['atlas'],
			'connectivity_metric': row['connectivity_metric'],
			'balanced_score': round(row['balanced_score'], 3),
			'original_score': round(row['quality_score'], 3),
			'score_change': round(row['balanced_score'] - row['quality_score'], 3),
			'rationale': row['scoring_rationale']
		})
    
	recommendations = {
		'methodology': 'Balanced optimization avoiding FA dominance',
		'summary': summary,
		'top_recommendations': top_10
	}
    
	output_json = f"balanced_recommendations_{timestamp}.json"
	with open(output_json, 'w') as f:
		json.dump(recommendations, f, indent=2)
	print(f"âœ… Saved recommendations: {output_json}")
    
	# Summary report
	output_report = f"balanced_optimization_report_{timestamp}.txt"
	with open(output_report, 'w') as f:
		f.write("BALANCED ATLAS-METRIC OPTIMIZATION REPORT\n")
		f.write("=" * 50 + "\n\n")
		f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
		f.write("METHODOLOGY\n")
		f.write("-----------\n")
		f.write("â€¢ FA penalty: 70% reduction (extreme effect sizes)\n")
		f.write("â€¢ COUNT/NCOUNT2 bonus: +20% (moderate effects)\n")
		f.write("â€¢ QA bonus: +10% (balanced anisotropy)\n")
		f.write("â€¢ Dominance penalty: Up to 60% for scores >0.9\n\n")
        
		f.write("SUMMARY STATISTICS\n")
		f.write("------------------\n")
		for key, value in summary.items():
			if key != 'top_metric_distribution':
				f.write(f"{key.replace('_', ' ').title()}: {value}\n")
		f.write(f"\nTop 20 Metric Distribution:\n")
		for metric, count in summary['top_metric_distribution'].items():
			f.write(f"  {metric}: {count}\n")
		f.write("\n")
        
		f.write("TOP BALANCED RECOMMENDATIONS\n")
		f.write("----------------------------\n")
		for rec in top_10:
			f.write(f"{rec['rank']}. {rec['atlas']} + {rec['connectivity_metric']}\n")
			f.write(f"   Balanced Score: {rec['balanced_score']} (was {rec['original_score']}, {rec['score_change']:+.3f})\n")
			f.write(f"   Rationale: {rec['rationale']}\n\n")
    
	print(f"âœ… Saved report: {output_report}")
    
	# Display top results
	print("\nğŸ† TOP 5 BALANCED RECOMMENDATIONS:")
	print("-" * 70)
	for rec in top_10[:5]:
		print(f"{rec['rank']}. {rec['atlas']} + {rec['connectivity_metric']}")
		print(f"   Score: {rec['balanced_score']} (was {rec['original_score']}, {rec['score_change']:+.3f})")
		print(f"   {rec['rationale']}")
		print()
    
	print(f"ğŸ“Š SUMMARY:")
	print(f"   â€¢ {summary['total_combinations']} combinations processed")
	print(f"   â€¢ {summary['fa_combinations']} FA combinations (penalized)")
	print(f"   â€¢ {summary['count_ncount2_combinations']} COUNT/NCOUNT2 combinations (favored)")
	print(f"   â€¢ Mean balanced score: {summary['mean_balanced_score']:.3f}")

if __name__ == "__main__":
	main()
EOF

# Run the balanced optimization
echo ""
echo "ğŸš€ Running balanced optimization..."
cd "$OUTPUT_DIR"
python "balanced_optimizer_${TIMESTAMP}.py" "../$OPTIMIZED_METRICS_FILE"

# Clean up temporary script
rm "balanced_optimizer_${TIMESTAMP}.py"


# --- Statistische Vergleichsplots generieren ---
echo ""
echo "ğŸ“Š Running statistical_metric_comparator.py for paired tests and violin plots..."
STAT_COMP_SCRIPT="$SCRIPT_DIR/statistical_metric_comparator.py"
if [ -f "$STAT_COMP_SCRIPT" ]; then
	mkdir -p "$OUTPUT_DIR"
	echo ""
	echo "ğŸ¯ Starte statistischen Vergleich mit gepaarten Tests und Visualisierung..."
	STAT_COMP_OUT=$(python "$STAT_COMP_SCRIPT" "../$OPTIMIZED_METRICS_FILE" "$OUTPUT_DIR" --plots 2>&1)
	# Extrahiere und zeige relevante Statistiken aus der Ausgabe
	echo "$STAT_COMP_OUT" | grep -E 'Total comparisons|Significant|Mean effect size|Large effects|Results saved to:|Created.*plots' || true
	echo "âœ… Statistische Vergleichsplots (inkl. Violin-Plots) gespeichert in: $OUTPUT_DIR"
	echo "   ğŸ“ˆ Verwendet gepaarte Tests (within-subject dependencies berÃ¼cksichtigt)"
	echo "   ğŸ» Violin-Plots zeigen Verteilungen mit Mittelwert/SD pro Atlas und Metrik"
else
	echo "âš ï¸  statistical_metric_comparator.py nicht gefunden, Ã¼berspringe Vergleichsplots."
fi

echo ""
echo "âœ… BALANCED OPTIMIZATION COMPLETED SUCCESSFULLY"
echo "==============================================="
echo ""
echo "ğŸ“ Results saved to: $OUTPUT_DIR/"
echo ""
echo "ğŸ¯ This balanced optimization replaces 03_optimize_metrics.sh"
echo "   It addresses FA dominance while maintaining scientific rigor."
echo ""
echo "ï¿½ STATISTICAL APPROACH:"
echo "   â€¢ Uses paired tests (within-subject dependencies)"
echo "   â€¢ Cohen's dz effect sizes for paired differences"
echo "   â€¢ MANOVA per atlas with PCA visualization"
echo "   â€¢ Violin plots show distributions + means/SD per group"
echo ""
echo "ï¿½ğŸ“‹ UPDATED WORKFLOW:"
echo "   1. âœ… 00_install.sh"
echo "   2. âœ… 01_extract_connectome.sh"
echo "   3. âœ… 03_balanced_optimizer.sh (THIS)"
echo "   4. âœ… 04_balanced_optimizer.sh (kept for compatibility)"
echo ""
echo "ğŸ† Ready for group comparisons with balanced atlas-metric combinations!"
