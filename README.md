# Braingraph Pipeline ğŸ§ 

A comprehensive neuroimaging connectivity analysis pipeline for processing DSI Studio fiber tracking data and performing network-based statistical analyses.

## ğŸ§  What it does

The Braingraph Pipeline is a **4-step automated workflow** that transforms raw DSI Studio fiber files into publication-ready network connectivity analyses:

1. **ğŸ”¬ Connectivity Extraction** - Extract connectivity matrices from DSI Studio files
2. **âš™ï¸ Network Quality Optimization** - Analyze and optimize network parameters 
3. **ğŸ¯ Quality-Based Selection** - Select optimal atlas/metric combinations
4. **ğŸ“Š Statistical Analysis** - Perform group comparisons and statistical testing

**Key Features:**
- ğŸ”„ **Automated end-to-end processing** from raw `.fz` files to statistical results
- ğŸ“Š **Multiple atlas support** (FreeSurfer, HCP-MMP, AAL3, Schaefer, etc.)
- ğŸ¯ **Quality-driven optimization** for reliable network metrics
- ğŸ§ª **Flexible testing framework** with JSON configurations
- ğŸ“ˆ **Built-in quality assurance** and outlier detection
- ğŸ” **Parameter sweep capabilities** for optimization studies

## ğŸš€ Quick Start

### Environment Setup

```bash
# Clone the repository
git clone https://github.com/MRI-Lab-Graz/braingraph-pipeline.git
cd braingraph-pipeline

# Set up the environment (RECOMMENDED)
./00_install_new.sh
source braingraph_pipeline/bin/activate

# Windows users
00_install_windows.bat
braingraph_pipeline\Scripts\activate.bat

# Validate setup (recommended)
python validate_setup.py --config configs/01_working_config.json
```

> ğŸ“‹ **Note**: See [INSTALLATION.md](INSTALLATION.md) for detailed installation instructions and platform-specific options.

### Basic Usage

```bash
```bash
# RECOMMENDED: Production run with cross-validated parameters  
python cross_validation_bootstrap_optimizer.py --data-dir /path/to/data --output-dir cv_results
python run_pipeline.py --cross-validated-config cross_validated_optimal_config.json --step all

# Alternative: Traditional approach with manual parameter selection
python run_pipeline.py --test-config configs/test_all_subjects.json --enable-bootstrap-qa

# Quick test with 5 subjects (development)
python run_pipeline.py --test-config configs/test_full_pipeline.json

# Individual steps (manual control)
python run_pipeline.py --step 01 --data-dir /path/to/data --extraction-config configs/optimal_config.json
python run_pipeline.py --test-config configs/bootstrap_qa_wave_2.json
python scripts/bootstrap_qa_validator.py validate bootstrap_results_*

# If QA validation passes, run full dataset
python run_pipeline.py --test-config configs/test_all_subjects.json --verbose

# Quick test for development (5 subjects)
python run_pipeline.py --test-config configs/test_full_pipeline.json --verbose
```

That's it! The pipeline will automatically:
- âœ… Extract connectivity matrices from DSI Studio files
- âœ… Optimize network quality metrics
- âœ… Select best atlas/metric combinations  
- âœ… Run statistical analysis
- âœ… Generate publication-ready results

## ï¿½ Pipeline Steps

### Step 01: Connectivity Extraction
**Script:** `scripts/extract_connectivity_matrices.py` or `./01_extract_connectome.sh`

Extracts connectivity matrices from DSI Studio fiber files using multiple atlases and connectivity metrics.

**Input:** 
- Raw DSI Studio files (`.fz`, `.fib.gz`)
- Configuration file (JSON)

**Output:**
- Organized connectivity matrices by atlas/metric
- Network measures (CSV files)
- Processing logs and batch summary

**Configuration Example:**
```json
{
  "atlases": ["FreeSurferDKT_Cortical", "HCP-MMP", "AAL3"],
  "connectivity_values": ["fa", "qa", "count", "ncount2"],
  "tract_count": 100000,
  "tracking_parameters": {
    "otsu_threshold": 0.4,
    "fa_threshold": 0.05,
    "min_length": 20
  }
}
```

### Step 02: Network Quality Optimization
**Script:** `scripts/metric_optimizer.py`

Analyzes network quality metrics and identifies optimal parameter combinations for reliable connectivity analysis.

**Features:**
- Sparsity analysis across atlas/metric combinations
- Small-world topology assessment
- Quality scoring based on network properties
- Reliability metrics calculation

**Output:**
- `optimized_metrics.csv` - Quality scores for all combinations
- `quality_analysis.json` - Detailed quality assessment
- Recommended parameter combinations

### Step 03: Quality-Based Selection
**Script:** `scripts/optimal_selection.py`

Selects optimal atlas/connectivity metric combinations based on quality assessments from Step 02.

**Selection Criteria:**
- Quality scores above defined thresholds
- Network topology properties (small-worldness, modularity)
- Cross-subject consistency
- Recommended combinations from optimization step

**Output:**
- `optimal_combinations.json` - Selected combinations with rationale
- `*_analysis_ready.csv` - Prepared datasets for statistical analysis
- `selection_summary.txt` - Detailed selection report

### Step 04: Statistical Analysis
**Script:** `scripts/statistical_analysis.py`

Performs comprehensive statistical comparisons and generates analysis reports.

**Features:**
- Group comparisons (e.g., patients vs. controls)
- Effect size calculations (Cohen's d)
- Multiple comparison corrections (FDR, Bonferroni)
- Automated reporting with visualizations

**Output:**
- `group_comparisons.csv` - Statistical test results
- `effect_sizes.csv` - Effect size calculations
- `analysis_report.html` - Comprehensive analysis report

## ğŸ“ Project Structure

The pipeline is organized into a clean, user-friendly structure:

```
braingraph-pipeline/
â”œâ”€â”€ run_pipeline.py                          # Main pipeline orchestrator
â”œâ”€â”€ cross_validation_bootstrap_optimizer.py  # Cross-validation optimization tool
â”œâ”€â”€ README.md                               # This documentation
â”œâ”€â”€ 00_install_new.sh                       # Installation script
â”œâ”€â”€ scripts/                                # Supporting processing scripts
â”‚   â”œâ”€â”€ extract_connectivity_matrices.py   # Connectivity extraction
â”‚   â”œâ”€â”€ metric_optimizer.py                # Network optimization
â”‚   â”œâ”€â”€ optimal_selection.py               # Quality-based selection
â”‚   â”œâ”€â”€ statistical_analysis.py            # Statistical analysis
â”‚   â”œâ”€â”€ bootstrap_qa_validator.py           # Bootstrap validation
â”‚   â””â”€â”€ json_validator.py                  # Configuration validation
â”œâ”€â”€ configs/                                # Configuration files
â”‚   â”œâ”€â”€ test_full_pipeline.json            # Test configurations
â”‚   â”œâ”€â”€ production_config.json             # Production settings
â”‚   â”œâ”€â”€ bootstrap_optimization_config.json  # Bootstrap optimization
â”‚   â””â”€â”€ ...                                # Other config files
â”œâ”€â”€ docs/                                   # Documentation
â”‚   â”œâ”€â”€ CONFIG_GUIDE.md                    # Configuration guide
â”‚   â”œâ”€â”€ INSTALLATION.md                    # Installation instructions
â”‚   â””â”€â”€ ...                                # Other documentation
â””â”€â”€ dsi_studio_tools/                       # DSI Studio integration tools
    â””â”€â”€ ...                                 # DSI Studio configurations
```

**Key Components:**
- **Main Scripts**: `run_pipeline.py` and `cross_validation_bootstrap_optimizer.py` remain in root for easy access
- **Supporting Scripts**: Organized in `scripts/` subdirectory
- **Configurations**: Centralized in `configs/` directory
- **Documentation**: Organized in `docs/` folder

## ğŸ§ª JSON Test Configuration System

The pipeline supports flexible testing through JSON configuration files that define data selection, pipeline parameters, and execution options.

### Test Configuration Files

**`test_full_pipeline.json`** - Standard test with subset of subjects
```json
{
  "test_config": {
    "name": "full_pipeline_test",
    "description": "Complete 4-step pipeline test with 5 subjects"
  },
  "data_selection": {
    "source_dir": "/path/to/your/data",
    "selection_method": "random",
    "n_subjects": 5
  },
  "pipeline_config": {
    "extraction_config": "optimal_config.json",
    "steps_to_run": ["01", "02", "03", "04"],
    "output_base_dir": "test_results"
  }
}
```

**`test_all_subjects.json`** - Production run with all subjects
```json
{
  "test_config": {
    "name": "production_analysis",
    "description": "Complete analysis of all available subjects"
  },
  "data_selection": {
    "source_dir": "/path/to/your/data",
    "n_subjects": "all",
    "selection_method": "first"
  },
  "pipeline_config": {
    "extraction_config": "optimal_config.json",
    "output_base_dir": "production_results",
    "steps_to_run": ["01", "02", "03", "04"]
  }
}
```

**`test_extraction_only.json`** - Test only connectivity extraction
```json
{
  "data_selection": {
    "n_subjects": 3,
    "selection_method": "first"
  },
  "pipeline_config": {
    "steps_to_run": ["01"],
    "extraction_config": "sweep_config.json"
  }
}
```

## ğŸ“Š Usage Examples

### Testing and Development

```bash
# Quick test with 3 subjects (extraction only)
python run_pipeline.py --test-config configs/test_extraction_only.json --verbose

# Complete pipeline test with 5 subjects
python run_pipeline.py --test-config configs/test_full_pipeline.json --verbose

# Test with specific configuration
python run_pipeline.py --test-config configs/custom_test.json --extraction-config configs/conservative_config.json

# Validate configuration before running
python scripts/json_validator.py configs/test_full_pipeline.json
```

### Production Runs

```bash
# Process all subjects with optimal parameters
python run_pipeline.py --test-config configs/test_all_subjects.json --verbose

# Use specific data directory and extraction configuration
python run_pipeline.py --data-dir /data/study1 --extraction-config configs/optimal_config.json

# Run analysis pipeline only (skip extraction)
python run_pipeline.py --step analysis --input organized_matrices/
```

### Individual Pipeline Steps

```bash
# Step 1: Extract connectivity matrices
python run_pipeline.py --step 01 --data-dir /data --extraction-config optimal_config.json

# Step 2: Optimize network parameters
python run_pipeline.py --step 02 --input organized_matrices/

# Step 3: Select optimal combinations
python run_pipeline.py --step 03 --input optimization_results/

# Step 4: Statistical analysis
python run_pipeline.py --step 04 --input selected_combinations/
```

### Legacy Workflow (Alternative)

```bash
# Traditional approach using shell scripts
./01_extract_connectome.sh data/ results/
python run_pipeline.py --input results/organized_matrices/ --output analysis/
```

## ï¿½ Output Structure

```
results/
â”œâ”€â”€ organized_matrices/              # Step 01 output
â”‚   â”œâ”€â”€ FreeSurferDKT_Cortical/
â”‚   â”‚   â”œâ”€â”€ count/
â”‚   â”‚   â”œâ”€â”€ fa/
â”‚   â”‚   â”œâ”€â”€ qa/
â”‚   â”‚   â””â”€â”€ ncount2/
â”‚   â”œâ”€â”€ HCP-MMP/
â”‚   â”œâ”€â”€ AAL3/
â”‚   â””â”€â”€ batch_processing_summary.json
â”œâ”€â”€ aggregated_network_measures.csv  # Compiled network measures
â”œâ”€â”€ optimization_results/            # Step 02 output
â”‚   â”œâ”€â”€ optimized_metrics.csv
â”‚   â”œâ”€â”€ quality_analysis.json
â”‚   â””â”€â”€ sparsity_analysis.csv
â”œâ”€â”€ selected_combinations/           # Step 03 output
â”‚   â”œâ”€â”€ optimal_combinations.json
â”‚   â”œâ”€â”€ FreeSurferDKT_Cortical_fa_analysis_ready.csv
â”‚   â”œâ”€â”€ HCP-MMP_count_analysis_ready.csv
â”‚   â””â”€â”€ selection_summary.txt
â””â”€â”€ statistical_results/             # Step 04 output
    â”œâ”€â”€ group_comparisons.csv
    â”œâ”€â”€ effect_sizes.csv
    â”œâ”€â”€ analysis_report.html
    â””â”€â”€ statistical_summary.json
```

## âš™ï¸ Configuration

### Main Config: `01_working_config.json`

```json
{
  "dsi_studio_cmd": "/path/to/dsi_studio",
  "atlases": ["AAL3", "HCP-MMP", "FreeSurferDKT_Cortical"],
  "connectivity_values": ["count", "fa", "qa", "ncount2"],
  "tract_count": 100000,
  "tracking_parameters": {
    "otsu_threshold": 0.6,
    "fa_threshold": 0.05,
    "min_length": 10
  }
}
```

### DSI Studio Configuration Files

The pipeline includes several pre-configured DSI Studio parameter sets:

- **`optimal_config.json`** - Optimized parameters from parameter sweeps
- **`conservative_config.json`** - Conservative tracking parameters for high precision
- **`liberal_config.json`** - Liberal tracking parameters for comprehensive coverage
- **`sweep_config.json`** - Parameter sweep configurations for optimization studies

### Parameter Sweeps

For tracking parameter optimization, configurations can include `sweep_parameters`:

```json
{
  "sweep_parameters": {
    "otsu_range": [0.3, 0.4, 0.5, 0.6],
    "min_length_range": [10, 20, 30],
    "fa_threshold_range": [0.05, 0.1, 0.15]
  }
}
```

## ğŸ› ï¸ Advanced Options

### Custom Data Selection

```json
{
  "data_selection": {
    "selection_method": "specific",
    "specific_subjects": ["sub-001.fz", "sub-002.fz", "sub-003.fz"],
    "exclude_subjects": ["sub-bad.fz"],
    "file_pattern": "*.fz",
    "random_seed": 42
  }
}
```

### Quality Control Options

```json
{
  "quality_checks": {
    "run_uniqueness_check": true,
    "run_outlier_analysis": true,
    "quality_thresholds": {
      "min_diversity_score": 0.05,
      "max_outlier_rate": 0.20,
      "min_sparsity": 0.05,
      "max_sparsity": 0.95
    }
  }
}
```

### Pipeline Execution Options

```json
{
  "pipeline_config": {
    "steps_to_run": ["01", "02", "03", "04"],
    "parallel_processing": true,
    "max_workers": 4,
    "cleanup_temp_files": true,
    "verbose_logging": true
  }
}
```

### Parameter Comparison Studies

Use different configurations to test parameter effects:

```bash
# Test conservative vs liberal parameters
python run_pipeline.py --test-config test_conservative.json
python run_pipeline.py --test-config test_liberal.json

# Compare results using built-in comparator
python statistical_metric_comparator.py conservative_results/ liberal_results/
```

## ğŸ§ª Advanced Features

### Quality-First Methodology

The pipeline implements a pure quality assessment approach:

- **Network topology metrics** (sparsity, small-worldness, modularity)
- **No statistical bias** in quality assessment
- **Reproducible selection** of optimal combinations
- **Cross-subject consistency** evaluation

### Enhanced Data Formats

- **Automatic CSV conversion** from MATLAB formats
- **Robust error handling** for missing dependencies  
- **Cross-platform compatibility** (macOS, Linux, Windows)
- **Memory-efficient processing** for large datasets

### Parameter Optimization

- **Intelligent parameter sweeps** for tracking optimization
- **Sparsity-based quality metrics**
- **Automated best parameter detection**
- **Multi-objective optimization** balancing multiple quality criteria

### Built-in Validation

The pipeline includes comprehensive validation tools:

```bash
# Validate JSON configurations
python json_validator.py your_config.json

# Check pipeline setup
python validate_setup.py --config 01_working_config.json

# Quick quality check of results
python quick_quality_check.py results/organized_matrices/
```

## ğŸ”§ Troubleshooting

### Common Issues

**DSI Studio not found:**
```bash
# Update path in configuration
{
  "dsi_studio_cmd": "/Applications/dsi_studio.app/Contents/MacOS/dsi_studio"
}
```

**Configuration validation errors:**
```bash
# Validate before running
python json_validator.py your_config.json
```

**Memory issues with large datasets:**
- Reduce `tract_count` in configuration (e.g., from 100000 to 50000)
- Use `n_subjects` limit for testing
- Process in smaller batches
- Increase system swap space

**Pipeline step failures:**
```bash
# Run individual steps to isolate issues
python run_pipeline.py --step 01 --verbose
python validate_setup.py --config your_config.json
```

**Missing dependencies:**
```bash
# Reinstall environment
./00_install_new.sh
source braingraph_pipeline/bin/activate
pip install -r requirements.txt
```

## ğŸ”¬ Cross-Validation Bootstrap Parameter Optimization

**NEW: Scientific Parameter Validation System**

The `cross_validation_bootstrap_optimizer.py` implements a scientifically robust approach to parameter optimization using cross-validation methodology:

### ğŸ¯ **How It Works:**

1. **Subject Splitting**: Randomly splits dataset into two validation subsets (5 subjects each)
2. **Parameter Sweep**: Both subsets test the same parameter combinations independently  
3. **Cross-Validation**: Validates that both subsets converge to identical optimal parameters
4. **Scientific Confidence**: Only proceeds if parameter consistency â‰¥ 80% (configurable)

### ğŸš€ **Usage:**

```bash
# STEP 1: Run cross-validation parameter optimization
python cross_validation_bootstrap_optimizer.py --data-dir /path/to/data --output-dir cv_results

# STEP 2: Use validated parameters for full analysis  
python run_pipeline.py --cross-validated-config cross_validated_optimal_config.json --step all
```

### âœ… **Validation Results (Latest Run):**

**Cross-Validation Performance:**
- âœ… **Wave 1 Optimal:** `FreeSurferDKT_Tissue + fa` (score: 1.000)
- âœ… **Wave 2 Optimal:** `FreeSurferDKT_Tissue + fa` (score: 1.000)
- âœ… **Consistency:** 100% agreement between validation waves
- âœ… **Validation Method:** 2-wave bootstrap with random subject selection (10 subjects each)

**Production Analysis Results:**
- âœ… **Final Optimal:** `FreeSurferDKT_Subcortical + fa` (score: 1.000)
- âœ… **Dataset:** All 52 subjects successfully processed
- âœ… **Quality Score:** Perfect validation (1.000)
- âœ… **Analysis-Ready Datasets:** 5 optimal combinations generated

**Recommended Configuration:**

```json
{
  "optimal_atlas": "FreeSurferDKT_Subcortical",
  "optimal_metric": "fa",
  "tract_count": 10000,
  "quality_score": 1.000,
  "validation_status": "cross-validated"
}
```

### âš™ï¸ **Configuration:**

The optimizer tests parameter combinations defined in `bootstrap_optimization_config.json`:

```json
{
  "parameter_sweep": {
    "parameters": {
      "track_count": [500000, 1000000, 2000000],      // Fiber count options
      "step_size": [0.25, 0.5, 1.0],                  // Tracking step size  
      "angular_threshold": [30, 45, 60],              // Maximum turning angle
      "fa_threshold": [0.1, 0.15, 0.2]               // FA termination threshold
    }
  }
}
```

### ğŸ“Š **Output:**

- **Parameter Consistency**: 100% = both waves found identical optimal parameters
- **Performance Correlation**: Measures score agreement between validation waves
- **Validated Config**: `cross_validated_optimal_config.json` ready for full pipeline

### ğŸ”§ **Execution Modes:**

**Demo Mode (Fast)**: Uses synthetic evaluation for workflow demonstration
**Production Mode**: Runs real DSI Studio processing (slower but scientifically valid)

*Note: Current implementation uses demo mode. For production, update `execute_parameter_sweep()` method.*

### Quality Assurance

The pipeline includes comprehensive built-in quality assurance:

**ğŸ”¬ Bootstrap QA Validation (RECOMMENDED)**
- **Integrated bootstrap sampling** - Automatic 20% sampling in 2 waves for production datasets
- **Statistical stability assessment** - Cross-validation using scikit-learn methods  
- **Confidence interval analysis** - Quantify measurement precision
- **Coefficient of variation scoring** - Assess metric reliability across samples
- **Automated decision making** - Proceed/adjust recommendations based on QA results

**ğŸ“Š Standard Quality Checks**
- **Parameter uniqueness validation** - Ensures diverse parameter combinations
- **Outlier detection** across subjects and metrics
- **Network topology verification** - Validates small-world properties
- **Data completeness checks** - Identifies missing or corrupted data
- **Sparsity range validation** - Ensures meaningful connectivity density

**ğŸ¯ Usage:**
```bash
# Enable bootstrap QA for production runs (RECOMMENDED)
python run_pipeline.py --test-config test_all_subjects.json --enable-bootstrap-qa

# Manual bootstrap QA validation (legacy approach)
python bootstrap_qa_validator.py create /path/to/data
python bootstrap_qa_validator.py validate results_*
```

## ğŸ“š Additional Tools

- **`json_validator.py`** - Validate configuration files (both pipeline and DSI Studio configs)
- **`quick_quality_check.py`** - Standalone quality analysis for results
- **`aggregate_network_measures.py`** - Combine network measures across subjects
- **`verify_parameter_uniqueness.py`** - Check parameter diversity in sweep results
- **`statistical_metric_comparator.py`** - Compare results between different parameter sets

## ğŸ”§ Development

### Adding New Atlases

Edit `01_working_config.json`:

```json
{
  "atlases": ["NewAtlas", "ExistingAtlas1", "ExistingAtlas2"]
}
```

### Custom Metrics

Add to connectivity values:

```json
{
  "connectivity_values": ["count", "fa", "custom_metric"]
}
```

### Extending Analysis

The modular design allows easy extension:

- Add new optimization criteria in `metric_optimizer.py`
- Extend selection logic in `optimal_selection.py`
- Add statistical models in `statistical_analysis.py`
- Create custom quality metrics in `quick_quality_check.py`

### Custom Test Configurations

Create your own test configurations:

```json
{
  "test_config": {
    "name": "my_custom_test",
    "description": "Custom analysis for specific research question"
  },
  "data_selection": {
    "source_dir": "/path/to/data",
    "n_subjects": 10,
    "selection_method": "random",
    "random_seed": 123
  },
  "pipeline_config": {
    "extraction_config": "custom_config.json",
    "steps_to_run": ["01", "02", "03"],
    "output_base_dir": "custom_results"
  }
}
```

## ğŸ“š Documentation

- **Pipeline Overview**: This README
- **DSI Studio Tools**: `dsi_studio_tools/README.md`
- **Parameter Guide**: `dsi_studio_tools/TRACK_COUNT_GUIDE.md`
- **Batch Processing**: `dsi_studio_tools/BATCH_PROCESSING_GUIDE.md`
- **Configuration Guide**: `CONFIG_GUIDE.md`
- **Migration Guide**: `MIGRATION.md`

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Test with sample data using test configurations
5. Update documentation if needed
6. Submit a pull request

### Development Guidelines

- Follow existing code style and documentation patterns
- Add tests for new features using the JSON test framework
- Validate configurations with `json_validator.py`
- Test with multiple atlases and parameter combinations
- Update relevant documentation

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

For issues and questions:

1. **Check the documentation** - Review this README and related guides
2. **Validate setup** - Run `python validate_setup.py --config your_config.json`  
3. **Check configuration** - Use `python json_validator.py your_config.json`
4. **Review log files** - Check pipeline logs for detailed error information
5. **Test with small dataset** - Use test configurations to isolate issues
6. **Open an issue** - Create a GitHub issue with detailed information

### Common Solutions

- **Configuration errors**: Use `json_validator.py` to check syntax and parameters
- **DSI Studio issues**: Verify installation and path in configuration
- **Memory problems**: Reduce `tract_count` or process in smaller batches
- **Missing data**: Check file paths and permissions
- **Quality issues**: Review quality check results and adjust thresholds

## ğŸ“§ Contact

**Research Group**: MRI Lab Graz  
**Repository**: [braingraph-pipeline](https://github.com/MRI-Lab-Graz/braingraph-pipeline)  
**Issues**: [GitHub Issues](https://github.com/MRI-Lab-Graz/braingraph-pipeline/issues)

## ğŸ”— Related Publications

*Add relevant publications here when available*

---

## ğŸ¯ Summary

The Braingraph Pipeline provides a complete solution for neuroimaging connectivity analysis:

- **ğŸ”„ End-to-end automation** from raw data to statistical results
- **ğŸ§ª Flexible testing** with JSON configuration system  
- **ğŸ“Š Quality-driven** analysis for reliable results
- **ğŸ› ï¸ Extensible design** for custom research needs
- **ğŸ“š Comprehensive documentation** and validation tools

## âš¡ Quick Reference

### Common Commands

```bash
# Bootstrap QA validation (RECOMMENDED)
python scripts/bootstrap_qa_validator.py create /path/to/data --qa-percentage 0.2 --n-waves 2

# Test with 5 subjects (development)
python run_pipeline.py --test-config configs/test_full_pipeline.json

# Production run with all subjects  
python run_pipeline.py --test-config configs/test_all_subjects.json

# Validate configuration
python scripts/json_validator.py configs/your_config.json

# Check pipeline setup
python validate_setup.py --config configs/01_working_config.json
```

### Available Test Configurations
- `configs/bootstrap_qa_wave_1.json` - Bootstrap QA validation wave 1 (20% of subjects)
- `configs/bootstrap_qa_wave_2.json` - Bootstrap QA validation wave 2 (20% of subjects)
- `configs/test_full_pipeline.json` - Complete 4-step test with 5 subjects
- `configs/test_all_subjects.json` - Production run with all available subjects
- `configs/test_extraction_only.json` - Test only connectivity extraction (Step 01)

### Key Configuration Files
- `configs/01_working_config.json` - DSI Studio extraction parameters
- `configs/optimal_config.json` - Optimized parameters from research
- `configs/sweep_config.json` - Parameter sweep configurations

**Ready to analyze brain connectivity? Start with `./00_install_new.sh` and test with `python run_pipeline.py --test-config configs/test_full_pipeline.json`!**
