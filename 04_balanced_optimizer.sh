#!/bin/bash
#
# 04_balanced_optimizer.sh - Balanced Atlas-Metric Optimization for Soccer Study
# =============================================================================
#
# This script replaces 03_optimize_metrics.sh with a balanced approach that:
# 1. Avoids FA dominance (extreme effect sizes)
# 2. Prioritizes moderate effect sizes (0.8-3.0 Cohen's d)
# 3. Favors COUNT/NCOUNT2 metrics for biological plausibility
# 4. Maintains scientific rigor for group comparisons
#
# Author: Generated for professional female soccer player connectivity study
# Date: 2025-08-07
#

set -e

# Configuration
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
OUTPUT_DIR="balanced_optimization_results"
TIMESTAMP=$(date "+%Y%m%d_%H%M%S")

echo "üéØ Balanced Atlas-Metric Optimization for Soccer Study"
echo "======================================================"
echo "Starting balanced optimization at $(date)"
echo ""

# Check prerequisites
echo "üìã Checking prerequisites..."

if [ ! -d "optimization_results" ]; then
    echo "‚ùå Error: No optimization_results directory found."
    echo "   Please run 03_optimize_metrics.sh first to generate base results."
    exit 1
fi

# Find the latest optimization results
LATEST_OPT_DIR=$(find optimization_results -name "optimization_*" -type d | sort | tail -1)
if [ -z "$LATEST_OPT_DIR" ]; then
    echo "‚ùå Error: No optimization results found in optimization_results/"
    exit 1
fi

OPTIMIZED_METRICS_FILE="$LATEST_OPT_DIR/optimized_metrics.csv"
if [ ! -f "$OPTIMIZED_METRICS_FILE" ]; then
    echo "‚ùå Error: optimized_metrics.csv not found in $LATEST_OPT_DIR"
    exit 1
fi

echo "‚úÖ Found optimization results: $LATEST_OPT_DIR"
echo "‚úÖ Using metrics file: $OPTIMIZED_METRICS_FILE"

# Create output directory
mkdir -p "$OUTPUT_DIR"

# Create the balanced optimization Python script inline
cat > "${OUTPUT_DIR}/balanced_optimizer_${TIMESTAMP}.py" << 'EOF'
#!/usr/bin/env python3
"""
Balanced Atlas-Metric Optimization (Generated by 04_balanced_optimizer.sh)
"""

import pandas as pd
import numpy as np
import sys
import json
from datetime import datetime
from pathlib import Path

def apply_balanced_scoring(df):
    """Apply balanced scoring that penalizes FA dominance."""
    
    df_balanced = df.copy()
    
    def calculate_balanced_score(row):
        original_score = row['quality_score']
        metric = row['connectivity_metric']
        
        # FA penalty (extreme effect sizes)
        fa_penalty = 0.3 if metric == 'fa' else 1.0
        
        # Dominance penalty for very high scores
        if original_score >= 0.95:
            dominance_penalty = 0.4
        elif original_score >= 0.90:
            dominance_penalty = 0.6
        elif original_score >= 0.80:
            dominance_penalty = 0.8
        else:
            dominance_penalty = 1.0
        
        # Balance bonus for moderate metrics
        if metric in ['count', 'ncount2']:
            balance_bonus = 1.2
        elif metric == 'qa':
            balance_bonus = 1.1
        else:
            balance_bonus = 1.0
        
        balanced_score = original_score * fa_penalty * dominance_penalty * balance_bonus
        return min(balanced_score, 1.0)
    
    df_balanced['balanced_score'] = df_balanced.apply(calculate_balanced_score, axis=1)
    
    # Add rationale
    def get_rationale(row):
        metric = row['connectivity_metric']
        original = row['quality_score']
        
        reasons = []
        if metric == 'fa':
            reasons.append("FA penalty (extreme effects)")
        elif metric in ['count', 'ncount2']:
            reasons.append("COUNT/NCOUNT2 bonus (moderate effects)")
        
        if original >= 0.95:
            reasons.append("High dominance penalty")
        elif original >= 0.90:
            reasons.append("Moderate dominance penalty")
        
        return "; ".join(reasons) if reasons else "Optimal range maintained"
    
    df_balanced['scoring_rationale'] = df_balanced.apply(get_rationale, axis=1)
    return df_balanced

def main():
    if len(sys.argv) != 2:
        print("Usage: python script.py <optimized_metrics.csv>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    
    # Load data
    print(f"üìä Loading optimization results from: {input_file}")
    df = pd.read_csv(input_file)
    print(f"   Original combinations: {len(df)}")
    
    # Apply balanced scoring
    print("‚öñÔ∏è  Applying balanced scoring...")
    df_balanced = apply_balanced_scoring(df)
    
    # Sort by balanced score
    df_sorted = df_balanced.sort_values('balanced_score', ascending=False)
    
    # Generate summary
    summary = {
        'total_combinations': len(df_balanced),
        'fa_combinations': len(df_balanced[df_balanced['connectivity_metric'] == 'fa']),
        'count_ncount2_combinations': len(df_balanced[df_balanced['connectivity_metric'].isin(['count', 'ncount2'])]),
        'mean_balanced_score': df_balanced['balanced_score'].mean(),
        'top_metric_distribution': df_sorted.head(20)['connectivity_metric'].value_counts().to_dict()
    }
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Detailed CSV
    output_csv = f"balanced_optimization_results_{timestamp}.csv"
    df_balanced.to_csv(output_csv, index=False)
    print(f"‚úÖ Saved detailed results: {output_csv}")
    
    # Top recommendations JSON
    top_10 = []
    for i, (_, row) in enumerate(df_sorted.head(10).iterrows()):
        top_10.append({
            'rank': i + 1,
            'atlas': row['atlas'],
            'connectivity_metric': row['connectivity_metric'],
            'balanced_score': round(row['balanced_score'], 3),
            'original_score': round(row['quality_score'], 3),
            'score_change': round(row['balanced_score'] - row['quality_score'], 3),
            'rationale': row['scoring_rationale']
        })
    
    recommendations = {
        'methodology': 'Balanced optimization avoiding FA dominance',
        'summary': summary,
        'top_recommendations': top_10
    }
    
    output_json = f"balanced_recommendations_{timestamp}.json"
    with open(output_json, 'w') as f:
        json.dump(recommendations, f, indent=2)
    print(f"‚úÖ Saved recommendations: {output_json}")
    
    # Summary report
    output_report = f"balanced_optimization_report_{timestamp}.txt"
    with open(output_report, 'w') as f:
        f.write("BALANCED ATLAS-METRIC OPTIMIZATION REPORT\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("METHODOLOGY\n")
        f.write("-----------\n")
        f.write("‚Ä¢ FA penalty: 70% reduction (extreme effect sizes)\n")
        f.write("‚Ä¢ COUNT/NCOUNT2 bonus: +20% (moderate effects)\n")
        f.write("‚Ä¢ QA bonus: +10% (balanced anisotropy)\n")
        f.write("‚Ä¢ Dominance penalty: Up to 60% for scores >0.9\n\n")
        
        f.write("SUMMARY STATISTICS\n")
        f.write("------------------\n")
        for key, value in summary.items():
            if key != 'top_metric_distribution':
                f.write(f"{key.replace('_', ' ').title()}: {value}\n")
        f.write(f"\nTop 20 Metric Distribution:\n")
        for metric, count in summary['top_metric_distribution'].items():
            f.write(f"  {metric}: {count}\n")
        f.write("\n")
        
        f.write("TOP BALANCED RECOMMENDATIONS\n")
        f.write("----------------------------\n")
        for rec in top_10:
            f.write(f"{rec['rank']}. {rec['atlas']} + {rec['connectivity_metric']}\n")
            f.write(f"   Balanced Score: {rec['balanced_score']} (was {rec['original_score']}, {rec['score_change']:+.3f})\n")
            f.write(f"   Rationale: {rec['rationale']}\n\n")
    
    print(f"‚úÖ Saved report: {output_report}")
    
    # Display top results
    print("\nüèÜ TOP 5 BALANCED RECOMMENDATIONS:")
    print("-" * 70)
    for rec in top_10[:5]:
        print(f"{rec['rank']}. {rec['atlas']} + {rec['connectivity_metric']}")
        print(f"   Score: {rec['balanced_score']} (was {rec['original_score']}, {rec['score_change']:+.3f})")
        print(f"   {rec['rationale']}")
        print()
    
    print(f"üìä SUMMARY:")
    print(f"   ‚Ä¢ {summary['total_combinations']} combinations processed")
    print(f"   ‚Ä¢ {summary['fa_combinations']} FA combinations (penalized)")
    print(f"   ‚Ä¢ {summary['count_ncount2_combinations']} COUNT/NCOUNT2 combinations (favored)")
    print(f"   ‚Ä¢ Mean balanced score: {summary['mean_balanced_score']:.3f}")

if __name__ == "__main__":
    main()
EOF

# Run the balanced optimization
echo ""
echo "üöÄ Running balanced optimization..."
cd "$OUTPUT_DIR"
python "balanced_optimizer_${TIMESTAMP}.py" "../$OPTIMIZED_METRICS_FILE"

# Clean up temporary script
rm "balanced_optimizer_${TIMESTAMP}.py"

echo ""
echo "‚úÖ BALANCED OPTIMIZATION COMPLETED SUCCESSFULLY"
echo "==============================================="
echo ""
echo "üìÅ Results saved to: $OUTPUT_DIR/"
echo ""
echo "üéØ This balanced optimization replaces 03_optimize_metrics.sh"
echo "   It addresses FA dominance while maintaining scientific rigor."
echo ""
echo "üìã UPDATED WORKFLOW:"
echo "   1. ‚úÖ 01_setup.sh"
echo "   2. ‚úÖ 02_compute_graph_metrics.sh"
echo "   3. ‚ùå 03_optimize_metrics.sh (DEPRECATED)"
echo "   4. ‚úÖ 04_balanced_optimizer.sh (NEW - USE THIS)"
echo ""
echo "üèÜ Ready for group comparisons with balanced atlas-metric combinations!"
